{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Yes it is possible to identify individuals either directly or indirectly from the dataset. If we have another dataset that have the comment style of different indivduals as the feature vector and the name of the individuals as the label, we can train a model that classifies comments. We can then run this dataset through the model and predict which individual made the given comment. <br /><br />\n",
    "ii. The dataset could be used to train a model that classifies the emotion of a comment based on the style of the comment. It can also be used to train a model that classifies the emotion of a comment based on the time the comment is made. <br /><br />\n",
    "iii. The dataset shouuld not be used to train a model that predicts the time the coment is made based on the coment style. It also shouldn't be feed into other models that would potentially violate the privacy of commenters, e.x their names, birthdates, gender, or other personal information.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. ['best', 'book', 'ever', 'it', 's', 'great'] <br /><br />\n",
    "b. d = 4855 <br /><br />\n",
    "c. Average number of non-zero features per comment in the training data: 12.241140215716486<br /><br />\n",
    "    The word appearing in the most number of comments: i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Hyperparameter Selection for a Linear_kernel SVM\n",
    "\n",
    "a. <br />\n",
    "By maintaining class proportions across folds, we are minimizing the deviation of folds away from the original training data. This way, the model each fold produces better reflects what a true model would be like if we train on the entire training data. <br /><br />\n",
    "\n",
    "b. \n",
    "| Performance Measures \t| C    \t| Performance        \t|\n",
    "|----------------------\t|------\t|--------------------\t|\n",
    "| Accuracy             \t| 1.0  \t| 0.9210263820957463 \t|\n",
    "| F1-Score             \t| 1.0  \t| 0.9202149609972474 \t|\n",
    "| AUROC                \t| 0.1  \t| 0.9723006785314479 \t|\n",
    "| Precision            \t| 0.01 \t| 0.9980048758644708 \t|\n",
    "| Sensitivity          \t| 1.0  \t| 0.9106326106326106 \t|\n",
    "| Specificity          \t| 1.0  \t| 0.9984615384615385 \t|\n",
    "\n",
    "Generally, the performance improves and reaches a peak and then either starts decaying or immediately drops to a lower value and stays reletively constant as we move from smaller to greater C values. I would use accuracy (C = 1.0) as the performance metrics to optimize for C because it is the most strict one: the set of labels predicted for a sample must exactly match the corresponding set of labels. This ensures that the model we train is rigorous.<br /><br />\n",
    "\n",
    "c.\n",
    "| Performance Measures \t| Performance        \t|\n",
    "|----------------------\t|--------------------\t|\n",
    "| Accuracy             \t| 0.9199384141647421 \t|\n",
    "| F1-Score             \t| 0.9204892966360856 \t|\n",
    "| AUROC                \t| 0.978608510133934 \t|\n",
    "| Precision            \t| 0.9922027290448343  \t|\n",
    "| Sensitivity          \t| 0.9261538461538461 \t|\n",
    "| Specificity          \t| 0.9137134052388289 \t|\n",
    "\n",
    "d.<br />\n",
    "![](Norm-l2_penalty.png)\n",
    "\n",
    "e. <br />\n",
    "![](Coefficient_vs_word.png)\n",
    "\n",
    "f. <br />\n",
    "Thank you guys for showing up at my mother's funeral. I am glad to have you as friends. Honestly, I miss her a lot, and it is quite sad that I didn't spend enough time with her. I feel so sorry; it must be disappointing for her.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Linear-Kernel SVM with L1 Penalty and Sqaured Hinge Loss\n",
    "\n",
    "a. <br />\n",
    "C value: 1.0 <br />\n",
    "The mean CV AUROC score: 0.9749627998715654 <br />\n",
    "AUROC test score: 0.9767618821856111 <br />\n",
    "\n",
    "b.<br />\n",
    "![](Norm-l1_penalty.png)\n",
    "\n",
    "c.<br />\n",
    "We observe that the norms for l2 penalty is much larger than that of l1, meaning that l1 penalty produces a much sparser theta for all C values. The gradient of l1 is constant to each element, meaning that all coefficients are reduced by the same amount so many unimportant features eventually has 0 as their weights to reduce the penalty. If we take the gradient of the minimization expression using l2, we find that it is linear to theta itself. This means that the model can balance the weights of different features and have a more complex parameter vector. \n",
    "\n",
    "d. <br />\n",
    "A squared hinge is forgiving for miscalssified data points that's within the margin comapred to hinge loss. However, it gives more penalty to miscalssified data points. Therefore, the margin will be wider when we use squared hinge loss to reduce the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Hyperparameter Selection for a Quadatic_Kernel SVM TOOOOOOOOOOOOOOOOOOOOOOOOOOODOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "\n",
    "a.\n",
    "| Tuning Scheme \t| C                 \t| r                  \t| AUROC              \t|\n",
    "|---------------\t|-------------------\t|--------------------\t|--------------------\t|\n",
    "| Grid Search   \t| 1.0               \t| 100.0              \t| 0.9724901418747572 \t|\n",
    "| Random Search \t| 41.87830236855281 \t| 1.9600598203396193 \t| 0.9720746270361655 \t|\n",
    "\n",
    "b. <br />\n",
    "Generally, if we fix one of C and r and let the other variable vary, the performance curve with respect to the variable always goes up, peaks, and then decays. As C increases, we will see such curves with absolutely better performance but earlier peaks with respect to r. As r increases, we will also see such curves with absolutely better performance but ealier peaks with respect to C. The pros of grid search would be that we can specify the combination of parameter we want by hard-coding. The cons is that if we have multiple parameters, this process becomes very time-consuming. The pros of random search is that we can quickly generate a lot of combinations using a random distribution function; however, we have limited control over the values of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Learning Non-linear Classifiers with a Linear-Kernel SVM\n",
    "\n",
    "a. <br />\n",
    "![](4.4a.jpeg)\n",
    "\n",
    "b. <br />\n",
    "Pros: Despite being efficient, using the kernel method may mean loss of information from the original feature mapping. For example, 4x^2 can be 2x*2x or (-2x)*(-2x). Using explicit feature mapping preserves the mapping. <br /><br />\n",
    "Cons: Feature mapping to such a high-dimensional space is extremely inefficient and can take a lot of resources to run. It is much easier to use kernel methods.<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Arbitrary class weights\n",
    "\n",
    "a. <br />\n",
    "This modification will assign different weights to each class, which are assigned based on the number of samples in each class. This means if W_n is much greater than W_p, then negative points will be given higher weights than positive points because the weighted penalty (W*C) of negative data points is much higher than the penalty of positive data points. This means that the model will do its best fitting the negative ones to avoid the high penalty. So negatively-labeled points are more likely to classified correctly by this model. <br /><br />\n",
    "\n",
    "b. <br />\n",
    "Setting W_n=0.25, W_p=1 and W_n=1, W_p=4 will result in the same ratio between weights assigned to positive points and negative point because the ratio of class weights are the same.This also means a smaller theta since the all weights are reduced. However, Using W_n=1, W_p=4 will results in higher absolute numerical weights for data points because the weighted penalty (W*C) for misclassifcation is much higher than the case of W_n=0.25, W_p=1. So the model will prioritize classifying data points correctly.\n",
    "\n",
    "c. <br />\n",
    "| Performance Measures \t| Performance        \t|\n",
    "|----------------------\t|--------------------\t|\n",
    "| Accuracy             \t| 0.6173979984603541 \t|\n",
    "| F1-Score             \t| 0.7218802462227196 \t|\n",
    "| AUROC                \t| 0.9628137963731183 \t|\n",
    "| Precision            \t| 0.5672823218997362 \t|\n",
    "| Sensitivity          \t| 0.9923076923076923 \t|\n",
    "| Specificity          \t| 0.24191063174114022 \t|\n",
    "\n",
    "d.<br />\n",
    "Specificity is affected the most and drops by a lot. AUROC is affted the least. Specificity is the most affected one because positively-labeled data points are given much higher weight, wh ich means that the model will prioritize classifying data points correctly. This comes at the cost of lower performance against negativelt-labeled points, which is the key factor of specificity. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Imbalanced data\n",
    "\n",
    "a.\n",
    "| Class Weights    \t| Performance Measures \t| Performance        \t|\n",
    "|------------------\t|----------------------\t|--------------------\t|\n",
    "| W_n = 1, W_p = 1 \t| Accuracy             \t| 0.7995079950799509 \t|\n",
    "| W_n = 1, W_p = 1 \t| F1-Score             \t| 0.8885850991114149 \t|\n",
    "| W_n = 1, W_p = 1 \t| AUROC                \t| 0.9484568192543652    |\n",
    "| W_n = 1, W_p = 1 \t| Precision            \t| 0.7995079950799509 \t|\n",
    "| W_n = 1, W_p = 1 \t| Sensitivity          \t| 1.0 \t|\n",
    "| W_n = 1, W_p = 1 \t| Specificity          \t| 0.0 \t|\n",
    "\n",
    "b. <br />\n",
    "Sensitivity and specificity are affected the most. Training on imbalanced data means that the model will be better at classifying data points that are more prevalent while being less accurate when predicting data points that are less prevalent in the training data set. As we can see, sensitivity reaches 1, which means that almost all positively-labeled points are orrectly classfied. Specificity goes to 0, which means that negatively-labeled points are predicted poorly. <br /><br />\n",
    "\n",
    "c.<br />\n",
    "Yes, this matches the intuition. We have a lot more positive points than negative points, and since the model is trainined with a lot of positively-labeled points, we have a lot of true positives and few false positives. This means that F1-Score won't fluctuate that much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Choosing appropraite class weights\n",
    "\n",
    "a. <br />\n",
    "The strategy is that we need to use the class weights to mitigate for the imbalance. Since the ratio between positive and negative points is 8:2, we will need to give more weight to negative points. On the other hand, AUROC is a great metric to use here because it's value is related to the model's confidence when classifying. This means that the value is still reflexive even when we use imbalanced data. Eventually we have W_p = 3 and W_n = 10 with a performance of 0.9657291175082586\n",
    "\n",
    "b. <br />\n",
    "| Class Weights    \t| Performance Measures \t| Performance        \t|\n",
    "|------------------\t|----------------------\t|--------------------\t|\n",
    "| W_n = 10, W_p = 3 | Accuracy             \t| 0.8597785977859779 \t|\n",
    "| W_n = 10, W_p = 3 | F1-Score             \t| 0.9043624161073825 \t|\n",
    "| W_n = 10, W_p = 3 | AUROC                \t| 0.9657291175082586    |\n",
    "| W_n = 10, W_p = 3 | Precision            \t| 0.9944649446494465 \t|\n",
    "| W_n = 10, W_p = 3 | Sensitivity          \t| 0.8292307692307692 \t|\n",
    "| W_n = 10, W_p = 3 | Specificity          \t| 0.9815950920245399 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 The ROC Curve NEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEED to be fixed\n",
    "![](ROC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Challenge\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff736c5c8260fecefcd908694e246a75f13a31c6b9c152fae9a522ac29de4dde"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
