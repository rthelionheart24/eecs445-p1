{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Yes it is possible to identify individuals either directly or indirectly from the dataset. If we have another dataset that have the comment style of different indivduals as the feature vector and the name of the individuals as the label, we can train a model that classifies comments. We can then run this dataset through the model and predict which individual made the given comment. <br /><br />\n",
    "ii. The dataset could be used to train a model that classifies the emotion of a comment based on the style of the comment. It can also be used to train a model that classifies the emotion of a comment based on the time the comment is made. <br /><br />\n",
    "iii. The dataset shouuld not be used to train a model that predicts the time the coment is made based on the coment style. It also shouldn't be feed into other models that would potentially violate the privacy of commenters, e.x their names, birthdates, gender, or other personal information.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. ['best', 'book', 'ever', 'it', 's', 'great'] <br /><br />\n",
    "b. d = 4855 <br /><br />\n",
    "c. Average number of non-zero features per comment in the training data: 12.136932305055698<br /><br />\n",
    "    The word appearing in the most number of comments: little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Hyperparameter Selection for a Linear_kernel SVM\n",
    "\n",
    "a. <br />\n",
    "By maintaining class proportions across folds, we are minimizing the deviation of folds away from the original training data. This way, the model each fold produces better reflects what a true model would be like if we train on the entire training data. <br /><br />\n",
    "\n",
    "b. \n",
    "| Performance Measures \t| C    \t| Performance        \t|\n",
    "|----------------------\t|------\t|--------------------\t|\n",
    "| Accuracy             \t| 1.0  \t| 0.9210263820957463 \t|\n",
    "| F1-Score             \t| 1.0  \t| 0.9202149609972474 \t|\n",
    "| AUROC                \t| 0.1  \t| 0.9723006785314479 \t|\n",
    "| Precision            \t| 0.01 \t| 0.9980048758644708 \t|\n",
    "| Sensitivity          \t| 1.0  \t| 0.9210263820957463 \t|\n",
    "| Specificity          \t| 1.0  \t| 0.9210263820957463 \t|\n",
    "\n",
    "Generally, the performance improves and reaches a peak and then decays again as we move from smaller to greater C values. I would use accuracy (C = 1.0) as the performance metrics to optimize for C because it is the most strict one: the set of labels predicted for a sample must exactly match the corresponding set of labels. This ensures that the model we train is rigorous.<br /><br />\n",
    "\n",
    "c.\n",
    "| Performance Measures \t| Performance        \t|\n",
    "|----------------------\t|--------------------\t|\n",
    "| Accuracy             \t| 0.5057736720554272 \t|\n",
    "| F1-Score             \t| 0.5620736698499318 \t|\n",
    "| AUROC                \t| 0.4927379400260757 \t|\n",
    "| Precision            \t| 0.484375  \t        |\n",
    "| Sensitivity          \t| 0.5057736720554272 \t|\n",
    "| Specificity          \t| 0.5057736720554272 \t|\n",
    "\n",
    "d.<br />\n",
    "![](Norm-l2_penalty.png)\n",
    "\n",
    "e. <br />\n",
    "![](Coefficient_vs_word.png)\n",
    "\n",
    "f. <br />\n",
    "Gladly you didn't miss the disappointing data points. Congrats and thanks! Thank you again for your time and sorry for any inconvenience.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Linear-Kernel SVM with L1 Penalty and Sqaured Hinge Loss\n",
    "\n",
    "a. <br />\n",
    "C value: 1.0 <br />\n",
    "The mean CV AUROC score: 0.9682126750588289 <br />\n",
    "AUROC test score: 0.974850924269529 <br />\n",
    "\n",
    "b.<br />\n",
    "![](Norm-l1_penalty.png)\n",
    "\n",
    "c.<br />\n",
    "We observe that the norms for l2 penalty is much larger than that of l1, meaning that l1 penalty produces a much sparser theta for all C values. The gradient of l1 is constant to each element, meaning that all coefficients are reduced by the same amount so many unimportant features eventually has 0 as their weights to reduce the penalty. If we take the gradient of the minimization expression using l2, we find that it is linear to theta itself. This means that the model can balance the weights of different features and have a more complex parameter vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Hyperparameter Selection for a Quadatic_Kernel SVM\n",
    "\n",
    "a.\n",
    "| Tuning Scheme \t| C                 \t| r                  \t| AUROC              \t|\n",
    "|---------------\t|-------------------\t|--------------------\t|--------------------\t|\n",
    "| Grid Search   \t| 1.0               \t| 100.0              \t| 0.9724901418747572 \t|\n",
    "| Random Search \t| 41.87830236855281 \t| 1.9600598203396193 \t| 0.9720746270361655 \t|\n",
    "\n",
    "b. <br />\n",
    "Generally, if we fix one of C and r and let the other variable vary, the performance curve with respect to the variable always goes up, peaks, and then decays. As C increases, we will see such curves with absolutely better performance but earlier peaks with respect to r. As r increases, we will also see such curves with absolutely better performance but ealier peaks with respect to C. The pros of grid search would be that we can specify the combination of parameter we want by hard-coding. The cons is that if we have multiple parameters, this process becomes very time-consuming. The pros of random search is that we can quickly generate a lot of combinations using a random distribution function; however, we have limited control over the values of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Learning Non-linear Classifiers with a Linear-Kernel SVM\n",
    "\n",
    "a. <br />\n",
    "![](4.4a.png)\n",
    "\n",
    "b. <br />\n",
    "Pros: Despite being efficient, using the kernel method may mean loss of information from the original feature mapping. For example, 4x^2 can be 2x*2x or (-2x)*(-2x).<br /><br />\n",
    "Cons: Feature mapping to such a high-dimensional space is extremely inefficient and can take a lot of resources to run<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Arbitrary class weights\n",
    "\n",
    "a. <br />\n",
    "This modification will assign different weights to each class, which are assigned based on the number of samples in each class. This means if W_n is much greater than W_p, then negative points will be given higher weights than positive points because the weighted penalty (W*C) of negative data points is much higher than the penalty of positive data points. This means that the model will do its best fitting the negative ones to avoid the high penalty. <br /><br />\n",
    "\n",
    "b. <br />\n",
    "Setting W_n=0.25, W_p=1 and W_n=1, W_p=4 will result in the same ratio between weights assigned to positive points and negative point because the ratio of class weights are the same. However, Using W_n=1, W_p=4 will results in higher absolute numerical weights for data points because the weighted penalty (W*C) for misclassifcation is much higher than the case of W_n=0.25, W_p=1.\n",
    "\n",
    "c. <br />\n",
    "| Performance Measures \t| Performance        \t|\n",
    "|----------------------\t|--------------------\t|\n",
    "| Accuracy             \t| 0.5019245573518091 \t|\n",
    "| F1-Score             \t| 0.6676938880328711 \t|\n",
    "| AUROC                \t| 0.5015408320493067 \t|\n",
    "| Precision            \t| 0.5011565150346955 \t|\n",
    "| Sensitivity          \t| 0.5019245573518091 \t|\n",
    "| Specificity          \t| 0.5019245573518091 \t|\n",
    "\n",
    "d.<br />\n",
    "F1-Score is affected the most by the new class weights because it indicates the average of F1-Score of each class with weighting in the multi-class and multi-label cases. Thus, when we change the class weights, the F1-Score will fluctuate a lot.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Imbalanced data\n",
    "\n",
    "a.\n",
    "| Class Weights    \t| Performance Measures \t| Performance        \t|\n",
    "|------------------\t|----------------------\t|--------------------\t|\n",
    "| W_n = 1, W_p = 1 \t| Accuracy             \t| 0.7995079950799509 \t|\n",
    "| W_n = 1, W_p = 1 \t| F1-Score             \t| 0.8885850991114149 \t|\n",
    "| W_n = 1, W_p = 1 \t| AUROC                \t| 0.5                \t|\n",
    "| W_n = 1, W_p = 1 \t| Precision            \t| 0.7995079950799509 \t|\n",
    "| W_n = 1, W_p = 1 \t| Sensitivity          \t| 0.7995079950799509 \t|\n",
    "| W_n = 1, W_p = 1 \t| Specificity          \t| 0.7995079950799509 \t|\n",
    "\n",
    "b. <br />\n",
    "AUROC is affected the least by training on imbalanced data. When we train on balanced data from 4.1c, AUROC has has a performance measure of 0.4927379400260757; now it is slightly increased to 0.5. F1-Score is affected the most by training on imbalanced data. When we train on balanced data from 4.1c, F1-Score has has a performance measure of 0.5620736698499318; now it is increased to 0.8885850991114149. <br /><br />\n",
    "\n",
    "d.<br />\n",
    "Yes, this matches the intuition. F1-Score is affected by the weight we assign to different label classes. Even though we didn't explicitly assign any label class more weight than the other, when we train on imbalanced data, we are implicitly giving more weight to the more prevalent label class because they appear in the training set more than the other labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Choosing appropraite class weights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff736c5c8260fecefcd908694e246a75f13a31c6b9c152fae9a522ac29de4dde"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
